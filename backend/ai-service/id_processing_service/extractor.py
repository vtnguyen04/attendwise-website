import os
import re
import json
import cv2
import threading
import numpy as np
from PIL import Image, ImageEnhance
import torch

from ultralytics import YOLO  # Import YOLO
# from paddleocr import TextDetection # Removed, replaced by YOLO

from vietocr.tool.predictor import Predictor
from vietocr.tool.config import Cfg
from threading import Thread


class ThreadWithReturnValue(Thread):
    def __init__(
        self, group=None, target=None, name=None, args=(), kwargs={}, Verbose=None
    ):
        Thread.__init__(self, group, target, name, args, kwargs)
        self._return = None

    def run(self):
        if self._target is not None:
            self._return = self._target(*self._args, **self._kwargs)

    def join(self, *args):
        Thread.join(self, *args)
        return self._return


CURRENT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# Kh·ªüi t·∫°o c√°c bi·∫øn global ƒë·ªÉ tr√°nh t·∫£i l·∫°i m√¥ h√¨nh
yolo_detector = None  # New global for YOLO model
text_recognizer = None

# --- C·∫§U H√åNH CHO YOLO V√Ä X·ª¨ L√ù TEXT ---
# Ng∆∞·ª°ng ƒë·ªô tin c·∫≠y cho m·ªói class. ƒê√¢y l√† c√°c v√≠ d·ª• v√† c·∫ßn ƒë∆∞·ª£c tinh ch·ªânh.
CLASS_CONFIDENCE_THRESHOLDS = {
    "current_place": 0.5,
    "dob": 0.6,
    "expire_date": 0.6,
    "features": 0.5,
    "finger_print": 0.7, 
    "gender": 0.6,
    "id": 0.7,
    "issue_date": 0.6,
    "name": 0.6,
    "nationality": 0.6,
    "origin_place": 0.5,
    "qr": 0.7,
}

# C√°c l·ªõp c√≥ th·ªÉ l√† ƒëa d√≤ng v√† c·∫ßn n·ªëi chu·ªói
MULTI_LINE_CLASSES = ["current_place", "origin_place", "features"]

# C√°c l·ªõp ƒë·∫°i di·ªán cho v√πng h√¨nh ·∫£nh/khu v·ª±c, kh√¥ng ph·∫£i vƒÉn b·∫£n ƒë·ªÉ OCR
IMAGE_REGION_CLASSES = ["finger_print", "qr"]

# Ng∆∞·ª°ng ƒë·ªô tin c·∫≠y m·∫∑c ƒë·ªãnh cho YOLO detections n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh c·ª• th·ªÉ cho m·ªói class
DEFAULT_YOLO_CONF_THRESHOLD = 0.25

# --- D·ªÆ LI·ªÜU ƒê·ªÇ X√ÅC TH·ª∞C N√ÇNG CAO ---
VIETNAMESE_SURNAMES = {
    "Nguy·ªÖn", "Tr·∫ßn", "L√™", "Ph·∫°m", "Ho√†ng", "Hu·ª≥nh", "Phan", "V≈©", "V√µ", "ƒê·∫∑ng", "B√πi", "ƒê·ªó", "H·ªì", "Ng√¥", "D∆∞∆°ng", "L√Ω", "ƒêo√†n", "ƒêinh", "L√¢m", "Tr·ªãnh", "ƒê√†o", "Mai", "L∆∞∆°ng",
}

VIETNAMESE_PROVINCES = {
    "H√† N·ªôi", "H·ªì Ch√≠ Minh", "H·∫£i Ph√≤ng", "ƒê√† N·∫µng", "C·∫ßn Th∆°", "An Giang", "B√† R·ªãa - V≈©ng T√†u", "B·∫Øc Giang", "B·∫Øc K·∫°n", "B·∫°c Li√™u", "B·∫Øc Ninh", "B·∫øn Tre", "B√¨nh ƒê·ªãnh", "B√¨nh D∆∞∆°ng", "B√¨nh Ph∆∞·ªõc", "B√¨nh Thu·∫≠n", "C√† Mau", "Cao B·∫±ng", "ƒê·∫Øk L·∫Øk", "ƒê·∫Øk N√¥ng", "ƒêi·ªán Bi√™n", "ƒê·ªìng Nai", "ƒê·ªìng Th√°p", "Gia Lai", "H√† Giang", "H√† Nam", "H√† Tƒ©nh", "H·∫£i D∆∞∆°ng", "H·∫≠u Giang", "H√≤a B√¨nh", "H∆∞ng Y√™n", "Kh√°nh H√≤a", "Ki√™n Giang", "Kon Tum", "Lai Ch√¢u", "L√¢m ƒê·ªìng", "L·∫°ng S∆°n", "L√†o Cai", "Long An", "Nam ƒê·ªãnh", "Ngh·ªá An", "Ninh B√¨nh", "Ninh Thu·∫≠n", "Ph√∫ Th·ªç", "Ph√∫ Y√™n", "Qu·∫£ng B√¨nh", "Qu·∫£ng Nam", "Qu·∫£ng Ng√£i", "Qu·∫£ng Ninh", "Qu·∫£ng Tr·ªã", "S√≥c TrƒÉng", "S∆°n La", "T√¢y Ninh", "Th√°i B√¨nh", "Th√°i Nguy√™n", "Thanh H√≥a", "Th·ª´a Thi√™n Hu·∫ø", "Ti·ªÅn Giang", "Tr√† Vinh", "Tuy√™n Quang", "Vƒ©nh Long", "Vƒ©nh Ph√∫c", "Y√™n B√°i",
}


class Extractor:
    def __init__(self):
        global yolo_detector, text_recognizer

        # --- 1. KH·ªûI T·∫†O C√ÅC M√î H√åNH OCR & YOLO ---
        if text_recognizer is None:
            print(">>> ƒêang kh·ªüi t·∫°o m√¥ h√¨nh VietOCR (Text Recognition)...")
            config = Cfg.load_config_from_name("vgg_seq2seq")
            weights_path = os.path.join(CURRENT_DIR, "weights", "seq1seq.pth")
            if not os.path.exists(weights_path):
                raise FileNotFoundError(
                    f"Kh√¥ng t√¨m th·∫•y file weights VietOCR t·∫°i: {weights_path}"
                )
            config["weights"] = weights_path
            config["cnn"]["pretrained"] = False
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            config["device"] = self.device
            text_recognizer = Predictor(config)
            print(f"‚úÖ VietOCR ƒë√£ s·∫µn s√†ng tr√™n thi·∫øt b·ªã {self.device}.")
        self.text_recognizer = text_recognizer

        if yolo_detector is None:
            print(">>> ƒêang kh·ªüi t·∫°o m√¥ h√¨nh YOLO (Object Detection)...")
            yolo_weights_path = os.path.join(CURRENT_DIR, "weights", "best.pt")
            if not os.path.exists(yolo_weights_path):
                raise FileNotFoundError(
                    f"Kh√¥ng t√¨m th·∫•y file YOLO weights t·∫°i: {yolo_weights_path}"
                )
            yolo_detector = YOLO(yolo_weights_path)
            print("‚úÖ YOLO (Object Detection) ƒë√£ s·∫µn s√†ng.")
        self.yolo_detector = yolo_detector

    def check_image_quality(self, image):
        """Ki·ªÉm tra c√°c v·∫•n ƒë·ªÅ c∆° b·∫£n v·ªÅ ch·∫•t l∆∞·ª£ng ·∫£nh tr∆∞·ªõc khi x·ª≠ l√Ω."""
        # 1. Ki·ªÉm tra ƒë·ªô m·ªù (blur)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        if laplacian_var < 60.0:  # Ng∆∞·ª°ng n√†y c√≥ th·ªÉ c·∫ßn tinh ch·ªânh
            return (
                False,
                f"·∫¢nh c√≥ th·ªÉ b·ªã m·ªù (ƒë·ªô n√©t: {laplacian_var:.2f}). Vui l√≤ng ch·ª•p l·∫°i.",
            )

        # 2. Ki·ªÉm tra ƒë·ªô l√≥a (glare)
        _, thresh = cv2.threshold(gray, 245, 255, cv2.THRESH_BINARY)
        glare_pixels = cv2.countNonZero(thresh)
        total_pixels = image.shape[0] * image.shape[1]
        glare_percentage = (glare_pixels / total_pixels) * 100
        if glare_percentage > 1:  # N·∫øu h∆°n 1% ·∫£nh b·ªã tr·∫Øng x√≥a
            return (
                False,
                f"·∫¢nh b·ªã l√≥a s√°ng ({glare_percentage:.2f}%). Vui l√≤ng tr√°nh ngu·ªìn s√°ng m·∫°nh.",
            )

        return True, "Ch·∫•t l∆∞·ª£ng ·∫£nh t·ªët."

    def find_and_crop_id_card(self, image):
        print("\n>>> B·∫Øt ƒë·∫ßu t√¨m khung th·∫ª trong ·∫£nh...")
        orig_image = image.copy()
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        edged = cv2.Canny(blurred, 50, 150)
        contours, _ = cv2.findContours(
            edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE
        )
        contours = sorted(contours, key=cv2.contourArea, reverse=True)[:5]
        screen_contour = None
        for c in contours:
            peri = cv2.arcLength(c, True)
            approx = cv2.approxPolyDP(c, 0.02 * peri, True)
            if len(approx) == 4:
                screen_contour = approx
                break
        if screen_contour is None:
            print(
                "‚ùå C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng vi·ªÅn 4 c·∫°nh. S·∫Ω x·ª≠ l√Ω to√†n b·ªô ·∫£nh."
            )
            return orig_image
        print("‚úÖ ƒê√£ t√¨m th·∫•y khung th·∫ª! ƒêang xoay v√† c·∫Øt...")
        points = screen_contour.reshape(4, 2)
        rect = np.zeros((4, 2), dtype="float32")
        s = points.sum(axis=1)
        rect[0] = points[np.argmin(s)]
        rect[2] = points[np.argmax(s)]
        diff = np.diff(points, axis=1)
        rect[1] = points[np.argmin(diff)]
        rect[3] = points[np.argmax(diff)]
        (tl, tr, br, bl) = rect
        widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
        widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
        maxWidth = max(int(widthA), int(widthB))
        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
        heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[0]) ** 2))
        maxHeight = max(int(heightA), int(heightB))
        dst = np.array(
            [
                [0, 0],
                [maxWidth - 1, 0],
                [maxWidth - 1, maxHeight - 1],
                [0, maxHeight - 1],
            ],
            dtype="float32",
        )
        M = cv2.getPerspectiveTransform(rect, dst)
        warped = cv2.warpPerspective(orig_image, M, (maxWidth, maxHeight))
        return warped

    def Detection(self, frame):
        print(f"\n>>> B·∫Øt ƒë·∫ßu ph√°t hi·ªán c√°c tr∆∞·ªùng th√¥ng tin tr√™n ·∫£nh b·∫±ng YOLO...")
        
        # Ch·∫°y YOLO model
        results = self.yolo_detector.predict(source=frame, conf=DEFAULT_YOLO_CONF_THRESHOLD, verbose=False)
        
        detected_items_raw = []
        for r in results:
            boxes = r.boxes.cpu().numpy()
            for i, box in enumerate(boxes):
                conf = box.conf[0]
                cls = int(box.cls[0])
                class_name = self.yolo_detector.names[cls]
                xyxy = box.xyxy[0] # [x1, y1, x2, y2]
                
                # Chuy·ªÉn ƒë·ªïi xyxy sang ƒë·ªãnh d·∫°ng polygon 4 ƒëi·ªÉm (top-left, top-right, bottom-right, bottom-left)
                box_points = [
                    [xyxy[0], xyxy[1]], # Top-left
                    [xyxy[2], xyxy[1]], # Top-right
                    [xyxy[2], xyxy[3]], # Bottom-right
                    [xyxy[0], xyxy[3]]  # Bottom-left
                ]
                
                # √Åp d·ª•ng ng∆∞·ª°ng ƒë·ªô tin c·∫≠y ri√™ng cho t·ª´ng class
                if conf >= CLASS_CONFIDENCE_THRESHOLDS.get(class_name, DEFAULT_YOLO_CONF_THRESHOLD):
                    detected_items_raw.append({
                        "box_points": box_points, 
                        "class_name": class_name, 
                        "confidence": conf
                    })
        
        # Nh√≥m c√°c detections theo class name
        # S·ª≠ d·ª•ng dict ƒë·ªÉ d·ªÖ d√†ng qu·∫£n l√Ω c√°c detections cho m·ªói class
        detections_by_class = {cls_name: [] for cls_name in self.yolo_detector.names.values()}
        for item in detected_items_raw:
            detections_by_class[item["class_name"]].append(item)

        final_detections_for_ocr = []
        
        for class_name, items in detections_by_class.items():
            if not items:
                continue

            if class_name in MULTI_LINE_CLASSES:
                # ƒê·ªëi v·ªõi c√°c class ƒëa d√≤ng, gi·ªØ l·∫°i t·∫•t c·∫£ c√°c detections ƒë√£ v∆∞·ª£t qua ki·ªÉm tra ƒë·ªô tin c·∫≠y ban ƒë·∫ßu
                # v√† s·∫Øp x·∫øp ch√∫ng theo t·ªça ƒë·ªô Y c·ªßa ƒëi·ªÉm tr√™n c√πng b√™n tr√°i ƒë·ªÉ n·ªëi chu·ªói sau n√†y
                sorted_items = sorted(items, key=lambda x: x["box_points"][0][1]) # S·∫Øp x·∫øp theo y c·ªßa ƒëi·ªÉm top-left
                final_detections_for_ocr.extend(sorted_items)
            elif class_name in IMAGE_REGION_CLASSES:
                # ƒê·ªëi v·ªõi c√°c class h√¨nh ·∫£nh, gi·ªØ l·∫°i t·∫•t c·∫£ c√°c box ƒë√£ ph√°t hi·ªán
                # (c√≥ th·ªÉ c√≥ nhi·ªÅu v√¢n tay/m√£ QR n·∫øu m√¥ h√¨nh ph√°t hi·ªán th·∫ø)
                final_detections_for_ocr.extend(items)
            else:
                # ƒê·ªëi v·ªõi c√°c class ch·ªâ n√™n xu·∫•t hi·ªán m·ªôt l·∫ßn, ch·ªçn c√°i c√≥ ƒë·ªô tin c·∫≠y cao nh·∫•t
                best_item = max(items, key=lambda x: x["confidence"])
                final_detections_for_ocr.append(best_item)

        if final_detections_for_ocr:
            print(f"‚úÖ Ph√°t hi·ªán ƒë∆∞·ª£c {len(final_detections_for_ocr)} tr∆∞·ªùng th√¥ng tin h·ª£p l·ªá.")
            # Debugging: In ra c√°c class ƒë∆∞·ª£c ph√°t hi·ªán
            for det in final_detections_for_ocr:
               print(f"  - Class: {det['class_name']}, Conf: {det['confidence']:.2f}")
        else:
            print("‚ùå C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y tr∆∞·ªùng th√¥ng tin n√†o.")
            
        # Tr·∫£ v·ªÅ danh s√°ch c√°c dictionary: {"box_points": ..., "class_name": ..., "confidence": ...}
        return final_detections_for_ocr


    def expand_box(self, box_points, frame_shape, expansion_factor=0.05):
        points = np.array(box_points, dtype=np.float32)
        center_x = np.mean(points[:, 0])
        center_y = np.mean(points[:, 1])
        expanded_points = []
        for point in points:
            dx = point[0] - center_x
            dy = point[1] - center_y
            new_x = center_x + dx * (1 + expansion_factor)
            new_y = center_y + dy * (1 + expansion_factor)
            new_x = max(0, min(frame_shape[1] - 1, new_x))
            new_y = max(0, min(frame_shape[0] - 1, new_y))
            expanded_points.append([new_x, new_y])
        return np.array(expanded_points, dtype=np.float32)

    def preprocess_image_for_rec(self, image_np):
        img_pil = Image.fromarray(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB))
        enhancer = ImageEnhance.Contrast(img_pil)
        img_pil = enhancer.enhance(1.2)
        enhancer = ImageEnhance.Sharpness(img_pil)
        img_pil = enhancer.enhance(1.4)
        return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

    def WarpAndRec(self, frame, detection_info): # ƒê√£ c·∫≠p nh·∫≠t signature
        box_points = detection_info["box_points"]
        class_name = detection_info["class_name"]

        if class_name in IMAGE_REGION_CLASSES:
            # ƒê·ªëi v·ªõi c√°c v√πng h√¨nh ·∫£nh, kh√¥ng c·∫ßn th·ª±c hi·ªán nh·∫≠n d·∫°ng vƒÉn b·∫£n
            print(f"üñºÔ∏è B·ªè qua nh·∫≠n d·∫°ng vƒÉn b·∫£n cho l·ªõp h√¨nh ·∫£nh: '{class_name}'")
            return [None, box_points, class_name] # VƒÉn b·∫£n l√† None cho c√°c v√πng h√¨nh ·∫£nh

        expanded_box = box_points
        rect = np.array(expanded_box, dtype="float32")
        (tl, tr, br, bl) = rect
        widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
        widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
        maxWidth = max(int(widthA), int(widthB))
        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
        heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
        maxHeight = max(int(heightA), int(heightB))
        if maxWidth == 0 or maxHeight == 0:
            print(f"‚ùå K√≠ch th∆∞·ªõc v√πng nh·∫≠n d·∫°ng cho '{class_name}' l√† 0 ho·∫∑c √¢m. B·ªè qua.")
            return ["", box_points, class_name]
        dst = np.array(
            [
                [0, 0],
                [maxWidth - 1, 0],
                [maxWidth - 1, maxHeight - 1],
                [0, maxHeight - 1],
            ],
            dtype="float32",
        )
        M = cv2.getPerspectiveTransform(rect, dst)
        matWarped = cv2.warpPerspective(frame, M, (maxWidth, maxHeight))
        
        matWarped = self.preprocess_image_for_rec(matWarped)
        try:
            recognized_text = self.text_recognizer.predict(Image.fromarray(matWarped))
            print(f"üìù Nh·∫≠n d·∫°ng ƒë∆∞·ª£c cho '{class_name}': '{recognized_text}'")
        except Exception as e:
            print(f"‚ùå L·ªói nh·∫≠n d·∫°ng vƒÉn b·∫£n cho '{class_name}': {e}")
            recognized_text = ""
        return [recognized_text, box_points, class_name] # Tr·∫£ v·ªÅ c·∫£ class_name

    def _reconstruct_text_from_ocr(self, raw_ocr_results):
        """
        T√°i c·∫•u tr√∫c vƒÉn b·∫£n t·ª´ c√°c k·∫øt qu·∫£ OCR ƒë√£ ƒë∆∞·ª£c g·∫Øn nh√£n l·ªõp.
        X·ª≠ l√Ω c√°c l·ªõp ƒëa d√≤ng v√† t·∫≠p h·ª£p k·∫øt qu·∫£ cho c√°c l·ªõp ƒë∆°n ho·∫∑c h√¨nh ·∫£nh.
        """
        structured_ocr_data = {}

        # Group raw results by class name
        temp_grouped_results = {}
        for text, box_points, class_name in raw_ocr_results:
            if class_name not in temp_grouped_results:
                temp_grouped_results[class_name] = []
            
            if class_name in IMAGE_REGION_CLASSES:
                # ƒê·ªëi v·ªõi c√°c v√πng h√¨nh ·∫£nh, ch·ªâ l∆∞u bounding box (d∆∞·ªõi d·∫°ng list [x_min, y_min, x_max, y_max])
                x_coords = [p[0] for p in box_points]
                y_coords = [p[1] for p in box_points]
                temp_grouped_results[class_name].append({
                    "bbox": [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]
                })
            else:
                # ƒê·ªëi v·ªõi c√°c v√πng vƒÉn b·∫£n, l∆∞u vƒÉn b·∫£n v√† t·ªça ƒë·ªô y c·ªßa ƒëi·ªÉm tr√™n c√πng b√™n tr√°i ƒë·ªÉ s·∫Øp x·∫øp
                if text and text.strip(): # Ch·ªâ th√™m n·∫øu vƒÉn b·∫£n kh√¥ng r·ªóng ho·∫∑c ch·ªâ to√†n kho·∫£ng tr·∫Øng
                    temp_grouped_results[class_name].append({
                        "text": text.strip(),
                        "y": min(p[1] for p in box_points),
                        "x": min(p[0] for p in box_points)
                    })
        
        for class_name, items in temp_grouped_results.items():
            if not items:
                structured_ocr_data[class_name] = None
                continue

            if class_name in MULTI_LINE_CLASSES:
                # S·∫Øp x·∫øp theo t·ªça ƒë·ªô y r·ªìi x cho c√°c tr∆∞·ªùng ƒëa d√≤ng
                sorted_items = sorted(items, key=lambda n: (n["y"], n["x"]))
                # N·ªëi chu·ªói vƒÉn b·∫£n, th√™m kho·∫£ng tr·∫Øng gi·ªØa c√°c d√≤ng, sau ƒë√≥ l√†m s·∫°ch
                concatenated_text = " ".join([item["text"] for item in sorted_items if "text" in item]).strip()
                # L√†m s·∫°ch ƒë∆°n gi·∫£n cho c√°c l·ªói OCR ho·∫∑c ƒë·ªãnh d·∫°ng trong ƒë·ªãa ch·ªâ ƒëa d√≤ng
                concatenated_text = re.sub(r'\s{2,}', ' ', concatenated_text) # Thay th·∫ø nhi·ªÅu kho·∫£ng tr·∫Øng b·∫±ng m·ªôt
                concatenated_text = re.sub(r'(\s*,\s*){2,}', ',', concatenated_text) # Thay th·∫ø nhi·ªÅu d·∫•u ph·∫©y b·∫±ng m·ªôt
                concatenated_text = concatenated_text.strip(',. ') # Lo·∫°i b·ªè c√°c d·∫•u ph·∫©y/ch·∫•m/kho·∫£ng tr·∫Øng th·ª´a ·ªü ƒë·∫ßu/cu·ªëi
                structured_ocr_data[class_name] = concatenated_text if concatenated_text else None
            elif class_name in IMAGE_REGION_CLASSES:
                # ƒê·ªëi v·ªõi c√°c v√πng h√¨nh ·∫£nh, tr·∫£ v·ªÅ danh s√°ch c√°c bounding box
                # (v√≠ d·ª•, c√≥ th·ªÉ c√≥ nhi·ªÅu v√¢n tay n·∫øu detect nhi·ªÅu l·∫ßn)
                structured_ocr_data[class_name] = [item["bbox"] for item in items]
            else:
                # ƒê·ªëi v·ªõi c√°c tr∆∞·ªùng ƒë∆°n d√≤ng, l·∫•y vƒÉn b·∫£n c·ªßa m·ª•c ƒë·∫ßu ti√™n (ho·∫∑c duy nh·∫•t)
                # (Gi·∫£ ƒë·ªãnh r·∫±ng ph∆∞∆°ng th·ª©c Detection ƒë√£ l·ªçc v√† ch·ªâ tr·∫£ v·ªÅ item t·ªët nh·∫•t)
                if items and "text" in items[0] and items[0]["text"].strip():
                    structured_ocr_data[class_name] = items[0]["text"].strip()
                else:
                    structured_ocr_data[class_name] = None
        
        print("\n>>> D·ªØ li·ªáu OCR ƒë√£ t√°i c·∫•u tr√∫c v√† ph√¢n lo·∫°i theo nh√£n YOLO:")
        for k, v in structured_ocr_data.items():
            if isinstance(v, str) and len(v) > 100:
                print(f"  - {k}: {v[:100]}...") # C·∫Øt b·ªõt chu·ªói d√†i ƒë·ªÉ hi·ªÉn th·ªã
            else:
                print(f"  - {k}: {v}")
        print("---\n")

        return structured_ocr_data

    def _extract_info_rule_based(self, structured_ocr_data, card_side):
        """
        Tr√≠ch xu·∫•t th√¥ng tin t·ª´ structured_ocr_data b·∫±ng c√°c quy t·∫Øc.
        Kh√¥ng s·ª≠ d·ª•ng m√¥ h√¨nh LLM.
        """
        extracted_data = {
            "ID_number": None,
            "Name": None,
            "Date_of_birth": None,
            "Gender": None,
            "Nationality": "Vi·ªát Nam",  # Qu·ªëc t·ªãch m·∫∑c ƒë·ªãnh l√† Vi·ªát Nam
            "Place_of_origin": None,
            "Place_of_residence": None,
            "Date_of_expiry": None,
            "Identifying_characteristics": None,
            "Date_of_issue": None,
            "Place_of_issue": None,
            "Finger_prints": [], # ƒê·ªÉ l∆∞u tr·ªØ bounding box c·ªßa v√¢n tay
            "QR_code_bbox": None # ƒê·ªÉ l∆∞u tr·ªØ bounding box c·ªßa m√£ QR
        }

        # X·ª≠ l√Ω c√°c class v√πng h√¨nh ·∫£nh tr∆∞·ªõc
        if "finger_print" in structured_ocr_data and structured_ocr_data["finger_print"]:
            extracted_data["Finger_prints"] = structured_ocr_data["finger_print"]
        if "qr" in structured_ocr_data and structured_ocr_data["qr"]:
            # N·∫øu c√≥ nhi·ªÅu m√£ QR, l·∫•y c√°i ƒë·∫ßu ti√™n ho·∫∑c k·∫øt h·ª£p t√πy logic
            extracted_data["QR_code_bbox"] = structured_ocr_data["qr"][0] if structured_ocr_data["qr"] else None


        # --- TR√çCH XU·∫§T TH√îNG TIN M·∫∂T TR∆Ø·ªöC ---
        if card_side == "front":
            extracted_data["ID_number"] = structured_ocr_data.get("id")
            
            name_raw = structured_ocr_data.get("name")
            if name_raw:
                # Chu·∫©n h√≥a t√™n: vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu, s·ª≠a l·ªói ph·ªï bi·∫øn
                name_parts = [word.capitalize() if word.upper() not in ["V√Ä", "TH·ªä"] else word for word in name_raw.split()]
                extracted_data["Name"] = " ".join(name_parts).replace('V√¥', 'V√µ').replace('vo', 'V√µ').replace('Vo', 'V√µ') # S·ª≠a l·ªói 'V√µ'

            extracted_data["Date_of_birth"] = structured_ocr_data.get("dob")
            extracted_data["Gender"] = structured_ocr_data.get("gender")
            if extracted_data["Gender"]:
                extracted_data["Gender"] = extracted_data["Gender"].capitalize().replace("NAM", "Nam").replace("N·ªÆ", "N·ªØ")
            
            # Qu·ªëc t·ªãch ƒë√£ m·∫∑c ƒë·ªãnh "Vi·ªát Nam", nh∆∞ng c√≥ th·ªÉ ghi ƒë√® n·∫øu YOLO t√¨m th·∫•y tr∆∞·ªùng "nationality"
            yolo_nationality = structured_ocr_data.get("nationality")
            if yolo_nationality:
                extracted_data["Nationality"] = yolo_nationality
            
            extracted_data["Place_of_origin"] = structured_ocr_data.get("origin_place")
            extracted_data["Place_of_residence"] = structured_ocr_data.get("current_place")
            extracted_data["Date_of_expiry"] = structured_ocr_data.get("expire_date")
            
            # Chu·∫©n h√≥a/l√†m s·∫°ch c∆° b·∫£n cho c√°c tr∆∞·ªùng ng√†y th√°ng
            if extracted_data["Date_of_birth"]:
                date_match_loose = re.search(r'(\d{1,2})[./\s-]?(\d{1,2})[./\s-]?(\d{4})', extracted_data["Date_of_birth"])
                if date_match_loose:
                    d, m, y = date_match_loose.groups()
                    extracted_data["Date_of_birth"] = f"{int(d):02d}/{int(m):02d}/{y}"
                else:
                    extracted_data["Date_of_birth"] = None 

            if extracted_data["Date_of_expiry"]:
                if "kh√¥ng th·ªùi h·∫°n" in extracted_data["Date_of_expiry"].lower() or "not exp" in extracted_data["Date_of_expiry"].lower():
                    extracted_data["Date_of_expiry"] = "Kh√¥ng th·ªùi h·∫°n"
                else:
                    date_match_loose = re.search(r'(\d{1,2})[./\s-]?(\d{1,2})[./\s-]?(\d{4})', extracted_data["Date_of_expiry"])
                    if date_match_loose:
                        d, m, y = date_match_loose.groups()
                        extracted_data["Date_of_expiry"] = f"{int(d):02d}/{int(m):02d}/{y}"
                    else:
                        extracted_data["Date_of_expiry"] = None


        # --- TR√çCH XU·∫§T TH√îNG TIN M·∫∂T SAU ---
        elif card_side == "back":
            extracted_data["Identifying_characteristics"] = structured_ocr_data.get("features")
            extracted_data["Date_of_issue"] = structured_ocr_data.get("issue_date")
            extracted_data["Place_of_issue"] = structured_ocr_data.get("issue_place") # T√™n class trong YOLO c√≥ th·ªÉ l√† 'issue_place' ho·∫∑c t∆∞∆°ng t·ª±

            # Chu·∫©n h√≥a 'Place_of_issue' n·∫øu n√≥ ch·ª©a c√°c m·∫´u chung
            if extracted_data["Place_of_issue"]:
                place_raw = extracted_data["Place_of_issue"].lower()
                if "c·ª•c tr∆∞·ªüng" in place_raw or "c·ª•c c·∫£nh s√°t" in place_raw or "c·∫£nh s√°t qu·∫£n l√Ω h√†nh ch√≠nh" in place_raw or "qlhc v·ªÅ ttxh" in place_raw or "b·ªô c√¥ng an" in place_raw:
                    extracted_data["Place_of_issue"] = "C·ª•c tr∆∞·ªüng C·ª•c C·∫£nh s√°t qu·∫£n l√Ω h√†nh ch√≠nh v·ªÅ tr·∫≠t t·ª± x√£ h·ªôi"
                extracted_data["Place_of_issue"] = extracted_data["Place_of_issue"].strip(',. ')
            
            if extracted_data["Date_of_issue"]:
                date_match_loose = re.search(r'(\d{1,2})[./\s-]?(\d{1,2})[./\s-]?(\d{4})', extracted_data["Date_of_issue"])
                if date_match_loose:
                    d, m, y = date_match_loose.groups()
                    extracted_data["Date_of_issue"] = f"{int(d):02d}/{int(m):02d}/{y}"
                else:
                    extracted_data["Date_of_issue"] = None

        return extracted_data

    def _perform_cross_validation(self, data):
        """
        X√°c th·ª±c ch√©o d·ªØ li·ªáu, ƒë·∫∑c bi·ªát l√† CCCD ID v·ªõi Ng√†y sinh v√† Gi·ªõi t√≠nh.
        """
        print(">>> B·∫Øt ƒë·∫ßu x√°c th·ª±c ch√©o d·ªØ li·ªáu (ID v√† Ng√†y sinh/Gi·ªõi t√≠nh)...")
        validation = {"passed": True, "errors": []}
        if not isinstance(data, dict):
            validation["passed"] = False
            validation["errors"].append("D·ªØ li·ªáu ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá.")
            return validation
        try:
            cccd_id = data.get("ID_number")
            dob_str = data.get("Date_of_birth")
            gender = data.get("Gender")

            if cccd_id:
                cccd_id_str = str(cccd_id).strip()
                if len(cccd_id_str) != 12 or not cccd_id_str.isdigit():
                    validation["passed"] = False
                    validation["errors"].append(
                        f"S·ªë CCCD '{cccd_id_str}' kh√¥ng h·ª£p l·ªá (ph·∫£i l√† 12 ch·ªØ s·ªë)."
                    )
                else:
                    # M√£ t·ªânh/th√†nh: 3 ch·ªØ s·ªë ƒë·∫ßu ti√™n
                    province_code = cccd_id_str[0:3]
                    # C√≥ th·ªÉ th√™m logic ki·ªÉm tra m√£ t·ªânh/th√†nh c√≥ h·ª£p l·ªá kh√¥ng n·∫øu c√≥ danh s√°ch m√£ t·ªânh/th√†nh
                    
                    # M√£ gi·ªõi t√≠nh v√† th·∫ø k·ª∑ sinh: ch·ªØ s·ªë th·ª© 4 (ch·ªâ s·ªë 3 trong chu·ªói)
                    gender_century_code = int(cccd_id_str[3])
                    
                    id_gender_from_code = None
                    expected_century_prefix = None

                    if gender_century_code == 0: id_gender_from_code = "Nam"; expected_century_prefix = "19" # Th·∫ø k·ª∑ 20, nam
                    elif gender_century_code == 1: id_gender_from_code = "N·ªØ"; expected_century_prefix = "19"  # Th·∫ø k·ª∑ 20, n·ªØ
                    elif gender_century_code == 2: id_gender_from_code = "Nam"; expected_century_prefix = "20" # Th·∫ø k·ª∑ 21, nam
                    elif gender_century_code == 3: id_gender_from_code = "N·ªØ"; expected_century_prefix = "20"  # Th·∫ø k·ª∑ 21, n·ªØ
                    elif gender_century_code == 4: id_gender_from_code = "Nam"; expected_century_prefix = "21" # Th·∫ø k·ª∑ 22, nam
                    elif gender_century_code == 5: id_gender_from_code = "N·ªØ"; expected_century_prefix = "21"  # Th·∫ø k·ª∑ 22, n·ªØ
                    else: 
                         validation["passed"] = False
                         validation["errors"].append(f"M√£ gi·ªõi t√≠nh/th·∫ø k·ª∑ '{gender_century_code}' trong CCCD kh√¥ng h·ª£p l·ªá (ngo√†i 0-5).")

                    # X√°c th·ª±c gi·ªõi t√≠nh t·ª´ CCCD ID v·ªõi Gi·ªõi t√≠nh ƒë√£ tr√≠ch xu·∫•t
                    extracted_gender_norm = gender.lower() if gender else ''
                    if extracted_gender_norm and id_gender_from_code: # Ch·ªâ so s√°nh n·∫øu c·∫£ hai ƒë·ªÅu c√≥ gi√° tr·ªã
                        if extracted_gender_norm != id_gender_from_code.lower():
                            validation["passed"] = False
                            validation["errors"].append(f"Gi·ªõi t√≠nh ƒë∆∞·ª£c tr√≠ch xu·∫•t ('{extracted_gender_norm}') kh√¥ng kh·ªõp v·ªõi m√£ gi·ªõi t√≠nh trong CCCD ID ('{id_gender_from_code}').")
                    elif not gender:
                        validation["errors"].append("Thi·∫øu th√¥ng tin Gi·ªõi t√≠nh ƒë·ªÉ x√°c th·ª±c ch√©o v·ªõi CCCD ID.")


                    # M√£ nƒÉm sinh: 2 ch·ªØ s·ªë cu·ªëi c·ªßa nƒÉm sinh t·ª´ CCCD ID
                    id_year_code = cccd_id_str[4:6]
                    if dob_str:
                        try:
                            dob_parts = dob_str.split('/')
                            if len(dob_parts) == 3 and len(dob_parts[2]) == 4:
                                dob_year = dob_parts[2]
                                if expected_century_prefix and dob_year[0:2] != expected_century_prefix:
                                     validation["errors"].append(f"Th·∫ø k·ª∑ sinh ({dob_year[0:2]}) kh√¥ng kh·ªõp v·ªõi m√£ gi·ªõi t√≠nh/th·∫ø k·ª∑ trong CCCD ID (m√£ {gender_century_code} -> th·∫ø k·ª∑ {expected_century_prefix}).")

                                if dob_year[2:] != id_year_code:
                                    validation["errors"].append(f"Hai ch·ªØ s·ªë cu·ªëi nƒÉm sinh ({dob_year[2:]}) kh√¥ng kh·ªõp v·ªõi CCCD ID ({id_year_code}).")

                            else:
                                validation["passed"] = False
                                validation["errors"].append("ƒê·ªãnh d·∫°ng nƒÉm sinh kh√¥ng h·ª£p l·ªá (ph·∫£i l√† dd/mm/yyyy).")
                        except ValueError:
                            validation["passed"] = False
                            validation["errors"].append(f"Ng√†y sinh '{dob_str}' kh√¥ng ·ªü ƒë·ªãnh d·∫°ng dd/mm/yyyy h·ª£p l·ªá.")
                    else:
                        validation["passed"] = False
                        validation["errors"].append("Thi·∫øu th√¥ng tin Ng√†y sinh ƒë·ªÉ x√°c th·ª±c ch√©o v·ªõi CCCD.")
            else:
                validation["passed"] = False
                validation["errors"].append("Thi·∫øu th√¥ng tin S·ªë CCCD ƒë·ªÉ x√°c th·ª±c ch√©o.")

        except Exception as e:
            validation["passed"] = False
            validation["errors"].append(f"L·ªói h·ªá th·ªëng khi x√°c th·ª±c ch√©o: {e}")

        if validation["passed"]:
            print("‚úÖ X√°c th·ª±c ch√©o th√†nh c√¥ng!")
        else:
            print(f"‚ùå C·∫¢NH B√ÅO: X√°c th·ª±c ch√©o th·∫•t b·∫°i! L·ªói: {validation['errors']}")
        data["cross_validation_status"] = validation
        return data

    def _perform_advanced_validation(self, data):
        """
        X√°c th·ª±c n√¢ng cao c√°c tr∆∞·ªùng th√¥ng tin nh∆∞ h·ªç v√† ƒë·ªãa danh.
        """
        print(">>> B·∫Øt ƒë·∫ßu x√°c th·ª±c n√¢ng cao (h·ªç, ƒë·ªãa danh)...")
        if not isinstance(data, dict):
            data["advanced_validation_warnings"] = ["D·ªØ li·ªáu ƒë·∫ßu v√†o kh√¥ng ph·∫£i dict."]
            return data

        warnings = []
        name = data.get("Name")
        if name and isinstance(name, str):
            # T√°ch t√™n th√†nh c√°c ph·∫ßn
            name_parts = name.strip().split(" ")

            if name_parts:
                # L·∫•y ph·∫ßn t·ª≠ ƒê·∫¶U TI√äN (l√† m·ªôt chu·ªói), sau ƒë√≥ m·ªõi .capitalize()
                surname = name_parts[0].capitalize()

                # So s√°nh v·ªõi danh s√°ch h·ªç ph·ªï bi·∫øn
                if surname not in VIETNAMESE_SURNAMES:
                    warnings.append(
                        f"H·ªç '{surname}' c√≥ v·∫ª kh√¥ng ph·ªï bi·∫øn ho·∫∑c kh√¥ng h·ª£p l·ªá."
                    )
            else:
                warnings.append("Kh√¥ng t√¨m th·∫•y H·ªç ƒë·ªÉ x√°c th·ª±c.")

        # Ph·∫ßn ki·ªÉm tra ƒë·ªãa danh
        origin = data.get("Place_of_origin", "")
        if origin and not any(province.lower() in origin.lower() for province in VIETNAMESE_PROVINCES):
            warnings.append(
                f"Qu√™ qu√°n '{origin}' kh√¥ng ch·ª©a t√™n t·ªânh/th√†nh ph·ªë h·ª£p l·ªá c·ªßa Vi·ªát Nam."
            )

        residence = data.get("Place_of_residence", "")
        if residence and not any(
            province.lower() in residence.lower() for province in VIETNAMESE_PROVINCES
        ):
            warnings.append(
                f"N∆°i th∆∞·ªùng tr√∫ '{residence}' kh√¥ng ch·ª©a t√™n t·ªânh/th√†nh ph·ªë h·ª£p l·ªá c·ªßa Vi·ªát Nam."
            )

        if not warnings:
            print("‚úÖ X√°c th·ª±c n√¢ng cao kh√¥ng c√≥ c·∫£nh b√°o.")
        else:
            print(f"‚ö†Ô∏è C·∫£nh b√°o t·ª´ x√°c th·ª±c n√¢ng cao: {warnings}")

        data["advanced_validation_warnings"] = warnings
        return data

    def GetInformationAndSave(self, extracted_results, card_side="front"): # Th√™m tham s·ªë card_side
        print(f"\n--- B·∫Øt ƒë·∫ßu quy tr√¨nh tr√≠ch xu·∫•t v√† x√°c th·ª±c th√¥ng tin cho m·∫∑t {card_side} ---")
        # extracted_results ·ªü ƒë√¢y l√† danh s√°ch c√°c [recognized_text, box_points, class_name]
        
        structured_ocr_data = self._reconstruct_text_from_ocr(extracted_results)
        
        if not structured_ocr_data:
            final_result = {"error": "Kh√¥ng c√≥ d·ªØ li·ªáu OCR n√†o ƒë∆∞·ª£c t√°i c·∫•u tr√∫c."}
        else:
            # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p tr√≠ch xu·∫•t d·ª±a tr√™n quy t·∫Øc
            extracted_data = self._extract_info_rule_based(structured_ocr_data, card_side)
            
            # Ki·ªÉm tra c∆° b·∫£n n·∫øu vi·ªác tr√≠ch xu·∫•t th·∫•t b·∫°i ho√†n to√†n
            if card_side == "front" and (not extracted_data.get("ID_number") and not extracted_data.get("Name")):
                final_result = {
                    "error": "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t th√¥ng tin c∆° b·∫£n m·∫∑t tr∆∞·ªõc (ID ho·∫∑c T√™n) b·∫±ng quy t·∫Øc."
                }
            elif card_side == "back" and (not extracted_data.get("Identifying_characteristics") and not extracted_data.get("Date_of_issue")):
                 final_result = {
                    "error": "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t th√¥ng tin c∆° b·∫£n m·∫∑t sau (ƒê·∫∑c ƒëi·ªÉm nh·∫≠n d·∫°ng ho·∫∑c Ng√†y c·∫•p) b·∫±ng quy t·∫Øc."
                }
            else:
                # Th·ª±c hi·ªán c√°c b∆∞·ªõc x√°c th·ª±c
                if card_side == "front": # Ch·ªâ x√°c th·ª±c ch√©o ID, Ng√†y sinh, Gi·ªõi t√≠nh cho m·∫∑t tr∆∞·ªõc
                    validated_data = self._perform_cross_validation(extracted_data)
                else:
                    validated_data = extracted_data # Kh√¥ng c√≥ x√°c th·ª±c ch√©o c·ª• th·ªÉ cho c√°c tr∆∞·ªùng m·∫∑t sau
                
                final_result = self._perform_advanced_validation(validated_data)

        try:
            output_filename = f"extracted_information_rule_based_{card_side}.json"
            output_path = os.path.join(
                CURRENT_DIR, output_filename
            )
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(final_result, f, indent=4, ensure_ascii=False)
            print(f"\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o file: {output_path}")
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u file: {e}")
        return final_result


idcard_extractor = Extractor()
if __name__ == "__main__":

    # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ·∫£nh ƒë√∫ng v·ªõi m√¥i tr∆∞·ªùng c·ªßa b·∫°n
    # V√≠ d·ª• cho m·∫∑t tr∆∞·ªõc
    img_path_front = os.path.join(CURRENT_DIR, "image/cccd1.jpg") 
    # V√≠ d·ª• cho m·∫∑t sau (n·∫øu c√≥, b·∫°n c√≥ th·ªÉ b·ªè comment v√† cung c·∫•p ƒë∆∞·ªùng d·∫´n)
    # img_path_back = os.path.join(CURRENT_DIR, "path/to/your/back_of_id_card.jpg") 

    # --- X·ª¨ L√ù M·∫∂T TR∆Ø·ªöC CƒÇN C∆Ø·ªöC C√îNG D√ÇN ---
    print("\n--- B·∫ÆT ƒê·∫¶U X·ª¨ L√ù M·∫∂T TR∆Ø·ªöC CƒÇN C∆Ø·ªöC C√îNG D√ÇN ---")
    original_frame_front = cv2.imread(img_path_front)
    if original_frame_front is None:
        raise FileNotFoundError(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh m·∫∑t tr∆∞·ªõc t·ª´ ƒë∆∞·ªùng d·∫´n: {img_path_front}")
    
    isok_front, mss_front = idcard_extractor.check_image_quality(original_frame_front)
    print(f"Ch·∫•t l∆∞·ª£ng ·∫£nh m·∫∑t tr∆∞·ªõc: {isok_front}, {mss_front}")
    
    if isok_front:
        frame_front = idcard_extractor.find_and_crop_id_card(original_frame_front)
        cv2.imwrite(os.path.join(CURRENT_DIR, "debug_01_cropped_card_front.jpg"), frame_front)

        # dt_polys_front b√¢y gi·ªù l√† danh s√°ch c√°c dictionary: {"box_points": ..., "class_name": ..., "confidence": ...}
        dt_polys_front = idcard_extractor.Detection(frame_front)

        print("\n>>> B·∫Øt ƒë·∫ßu nh·∫≠n d·∫°ng vƒÉn b·∫£n m·∫∑t tr∆∞·ªõc ƒëa lu·ªìng...")
        extracted_results_front = []
        threads_front = []
        for detection_info in dt_polys_front: # L·∫∑p qua c√°c dictionary detection_info
            t = ThreadWithReturnValue(
                target=idcard_extractor.WarpAndRec,
                args=(frame_front, detection_info), # Truy·ªÅn to√†n b·ªô dictionary detection_info
            )
            threads_front.append(t)
            t.start()

        for t in threads_front:
            extracted_results_front.append(t.join())

        # Truy·ªÅn "front" v√†o GetInformationAndSave
        info_front = idcard_extractor.GetInformationAndSave(extracted_results_front, card_side="front")

        print("\n--- K·∫æT QU·∫¢ CU·ªêI C√ôNG M·∫∂T TR∆Ø·ªöC ---")
        print(json.dumps(info_front, indent=2, ensure_ascii=False))
    else:
        print(f"‚ùå Kh√¥ng x·ª≠ l√Ω m·∫∑t tr∆∞·ªõc do ch·∫•t l∆∞·ª£ng ·∫£nh k√©m: {mss_front}")

    # --- X·ª¨ L√ù M·∫∂T SAU (T√πy ch·ªçn, n·∫øu b·∫°n b·ªè comment img_path_back) ---
    # print("\n--- B·∫ÆT ƒê·∫¶U X·ª¨ L√ù M·∫∂T SAU CƒÇN C∆Ø·ªöC C√îNG D√ÇN ---")
    # try:
    #     original_frame_back = cv2.imread(img_path_back)
    #     if original_frame_back is None:
    #         print(f"C·∫£nh b√°o: Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh m·∫∑t sau t·ª´ ƒë∆∞·ªùng d·∫´n: {img_path_back}")
    #     else:
    #         isok_back, mss_back = idcard_extractor.check_image_quality(original_frame_back)
    #         print(f"Ch·∫•t l∆∞·ª£ng ·∫£nh m·∫∑t sau: {isok_back}, {mss_back}")
            
    #         if isok_back:
    #             frame_back = idcard_extractor.find_and_crop_id_card(original_frame_back)
    #             cv2.imwrite(os.path.join(CURRENT_DIR, "debug_01_cropped_card_back.jpg"), frame_back)

    #             dt_polys_back = idcard_extractor.Detection(frame_back)

    #             print("\n>>> B·∫Øt ƒë·∫ßu nh·∫≠n d·∫°ng vƒÉn b·∫£n m·∫∑t sau ƒëa lu·ªìng...")
    #             extracted_results_back = []
    #             threads_back = []
    #             for detection_info in dt_polys_back:
    #                 t = ThreadWithReturnValue(
    #                     target=idcard_extractor.WarpAndRec,
    #                     args=(frame_back, detection_info),
    #                 )
    #                 threads_back.append(t)
    #                 t.start()

    #             for t in threads_back:
    #                 extracted_results_back.append(t.join())

    #             # Truy·ªÅn "back" v√†o GetInformationAndSave
    #             info_back = idcard_extractor.GetInformationAndSave(extracted_results_back, card_side="back")

    #             print("\n--- K·∫æT QU·∫¢ CU·ªêI C√ôNG M·∫∂T SAU ---")
    #             print(json.dumps(info_back, indent=2, ensure_ascii=False))
    #         else:
    #             print(f"‚ùå Kh√¥ng x·ª≠ l√Ω m·∫∑t sau do ch·∫•t l∆∞·ª£ng ·∫£nh k√©m: {mss_back}")
    # except FileNotFoundError:
    #     print("B·ªè qua x·ª≠ l√Ω m·∫∑t sau v√¨ ƒë∆∞·ªùng d·∫´n ·∫£nh kh√¥ng h·ª£p l·ªá ho·∫∑c kh√¥ng ƒë∆∞·ª£c cung c·∫•p.")